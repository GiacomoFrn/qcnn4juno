{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_quantum as tfq\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow import keras\n",
    "\n",
    "import cirq\n",
    "import sympy\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "# visualization tools\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from cirq.contrib.svg import SVGCircuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 5000    # Size of the train dataset\n",
    "n_valid = 150\n",
    "n_test  = 2000    # Size of the test dataset\n",
    "\n",
    "\n",
    "mnist_dataset = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist_dataset.load_data()\n",
    "\n",
    "# Reduce dataset size\n",
    "train_images = train_images[:n_train]\n",
    "train_labels = train_labels[:n_train]\n",
    "test_images = test_images[:n_test]\n",
    "test_labels = test_labels[:n_test]\n",
    "\n",
    "# Normalize pixel values within 0 and 1\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255\n",
    "\n",
    "# Add extra dimension for convolution channels\n",
    "train_images = np.array(train_images[..., tf.newaxis])\n",
    "test_images = np.array(test_images[..., tf.newaxis])\n",
    "\n",
    "# reduce image resolution\n",
    "train_images = tf.image.resize(train_images[:], (10,10)).numpy()\n",
    "test_images = tf.image.resize(test_images[:], (10,10)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QConv(tf.keras.layers.Layer):\n",
    "    def __init__(self, filter_size, depth, activation=None, name=None, kernel_regularizer=None, **kwangs):\n",
    "        super(QConv, self).__init__(name=name, **kwangs)\n",
    "        self.filter_size = filter_size\n",
    "        self.depth = depth\n",
    "        self.learning_params = []\n",
    "        self.QCNN_layer_gen()\n",
    "        # self.circuit_tensor = tfq.convert_to_tensor([self.circuit])\n",
    "        self.activation = tf.keras.layers.Activation(activation)\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "\n",
    "    def _next_qubit_set(self, original_size, next_size, qubits):\n",
    "        step = original_size // next_size\n",
    "        qubit_list = []\n",
    "        for i in range(0, original_size, step):\n",
    "            for j in range(0, original_size, step):\n",
    "                qubit_list.append(qubits[original_size*i + j])\n",
    "        return qubit_list\n",
    "\n",
    "    def _get_new_param(self):\n",
    "        \"\"\"\n",
    "        return new learnable parameter\n",
    "        all returned parameter saved in self.learning_params\n",
    "        \"\"\"\n",
    "        new_param = sympy.symbols(\"p\"+str(len(self.learning_params)))\n",
    "        self.learning_params.append(new_param)\n",
    "        return new_param\n",
    "    \n",
    "    def _QConv(self, step, target, qubits):\n",
    "        \"\"\"\n",
    "        apply learnable gates each quantum convolutional layer level\n",
    "        \"\"\"\n",
    "        yield cirq.CZPowGate(exponent=self._get_new_param())(qubits[target], qubits[target+step])\n",
    "        yield cirq.CXPowGate(exponent=self._get_new_param())(qubits[target], qubits[target+step])\n",
    "    \n",
    "    def QCNN_layer_gen(self):\n",
    "        \"\"\"\n",
    "        make quantum convolutional layer in QConv layer\n",
    "        \"\"\"\n",
    "        pixels = self.filter_size**2\n",
    "        # filter size: 2^n only for this version!\n",
    "        if np.log2(pixels) % 1 != 0:\n",
    "            raise NotImplementedError(\"filter size: 2^n only available\")\n",
    "        cirq_qubits = cirq.GridQubit.rect(self.filter_size, self.filter_size)\n",
    "        # mapping input data to circuit\n",
    "        input_circuit = cirq.Circuit()\n",
    "        input_params = [sympy.symbols('a%d' %i) for i in range(pixels)]\n",
    "        for i, qubit in enumerate(cirq_qubits):\n",
    "            input_circuit.append(cirq.rx(np.pi*input_params[i])(qubit))\n",
    "        # apply learnable gate set to QCNN circuit\n",
    "        QCNN_circuit = cirq.Circuit()\n",
    "        step_size = [2**i for i in range(np.log2(pixels).astype(np.int32))]\n",
    "        for step in step_size:\n",
    "            for target in range(0, pixels, 2*step):\n",
    "                QCNN_circuit.append(self._QConv(step, target, cirq_qubits))\n",
    "        # merge the circuits\n",
    "        full_circuit = cirq.Circuit()\n",
    "        full_circuit.append(input_circuit)\n",
    "        full_circuit.append(QCNN_circuit)\n",
    "        self.circuit = full_circuit # save circuit to the QCNN layer obj.\n",
    "        self.params = input_params + self.learning_params\n",
    "        self.op = cirq.Z(cirq_qubits[0])\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.width = input_shape[1]\n",
    "        self.height = input_shape[2]\n",
    "        self.channel = input_shape[3]\n",
    "        self.num_x = self.width - self.filter_size + 1\n",
    "        self.num_y = self.height - self.filter_size + 1\n",
    "        \n",
    "        self.kernel = self.add_weight(name=\"kenel\", \n",
    "                                      shape=[self.depth, \n",
    "                                             self.channel, \n",
    "                                             len(self.learning_params)],\n",
    "                                     initializer=tf.keras.initializers.glorot_normal(),\n",
    "                                     regularizer=self.kernel_regularizer)\n",
    "        self.circuit_tensor = tfq.convert_to_tensor([self.circuit] * self.num_x * self.num_y * self.channel)\n",
    "    0.2\n",
    "    def call(self, inputs):\n",
    "        # input shape: [N, width, height, channel]\n",
    "        # slide and collect data\n",
    "        stack_set = None\n",
    "        for i in range(self.num_x):\n",
    "            for j in range(self.num_y):\n",
    "                slice_part = tf.slice(inputs, [0, i, j, 0], [-1, self.filter_size, self.filter_size, -1])\n",
    "                slice_part = tf.reshape(slice_part, shape=[-1, 1, self.filter_size, self.filter_size, self.channel])\n",
    "                if stack_set == None:\n",
    "                    stack_set = slice_part\n",
    "                else:\n",
    "                    stack_set = tf.concat([stack_set, slice_part], 1)  \n",
    "        # -> shape: [N, num_x*num_y, filter_size, filter_size, channel]\n",
    "        stack_set = tf.transpose(stack_set, perm=[0, 1, 4, 2, 3])\n",
    "        # -> shape: [N, num_x*num_y, channel, filter_size, fiter_size]\n",
    "        stack_set = tf.reshape(stack_set, shape=[-1, self.filter_size**2])\n",
    "        # -> shape: [N*num_x*num_y*channel, filter_size^2]\n",
    "        \n",
    "        # total input citcuits: N * num_x * num_y * channel\n",
    "        circuit_inputs = tf.tile([self.circuit_tensor], [tf.shape(inputs)[0], 1])\n",
    "        circuit_inputs = tf.reshape(circuit_inputs, shape=[-1])\n",
    "        tf.fill([tf.shape(inputs)[0]*self.num_x*self.num_y, 1], 1)\n",
    "        outputs = []\n",
    "        for i in range(self.depth):\n",
    "            controller = tf.tile(self.kernel[i], [tf.shape(inputs)[0]*self.num_x*self.num_y, 1])\n",
    "            outputs.append(self.single_depth_QCNN(stack_set, controller, circuit_inputs))\n",
    "            # shape: [N, num_x, num_y] \n",
    "            \n",
    "        output_tensor = tf.stack(outputs, axis=3)\n",
    "        output_tensor = tf.math.acos(tf.clip_by_value(output_tensor, -1+1e-5, 1-1e-5)) / np.pi\n",
    "        # output_tensor = tf.clip_by_value(tf.math.acos(output_tensor)/np.pi, -1, 1)\n",
    "        return self.activation(output_tensor)\n",
    "    \n",
    "    def single_depth_QCNN(self, input_data, controller, circuit_inputs):\n",
    "        \"\"\"\n",
    "        make QCNN for 1 channel only\n",
    "        \"\"\"\n",
    "        # input shape: [N*num_x*num_y*channel, filter_size^2]\n",
    "        # controller shape: [N*num_x*num_y*channel, len(learning_params)]\n",
    "        input_data = tf.concat([input_data, controller], 1)\n",
    "        # input_data shape: [N*num_x*num_y*channel, len(learning_params)]\n",
    "        QCNN_output = tfq.layers.Expectation()(circuit_inputs, \n",
    "                                               symbol_names=self.params,\n",
    "                                               symbol_values=input_data,\n",
    "                                               operators=self.op)\n",
    "        # QCNN_output shape: [N*num_x*num_y*channel]\n",
    "        QCNN_output = tf.reshape(QCNN_output, shape=[-1, self.num_x, self.num_y, self.channel])\n",
    "        return tf.math.reduce_sum(QCNN_output, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = np.shape(train_images)[1]\n",
    "height = np.shape(train_images)[2]\n",
    "\n",
    "qcnn_model = models.Sequential()\n",
    "\n",
    "\n",
    "qcnn_model.add(QConv(filter_size=2, depth=8, activation='relu', \n",
    "                     name='qconv1', input_shape=(width, height, 1)))\n",
    "#model.add(layers.Conv2D(16, (2, 2), activation='relu'))\n",
    "qcnn_model.add(layers.Flatten())\n",
    "qcnn_model.add(layers.Dense(32, activation='relu'))\n",
    "qcnn_model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 23/700 [..............................] - ETA: 2:56 - loss: 0.2424 - accuracy: 0.9130"
     ]
    }
   ],
   "source": [
    "qcnn_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "qcnn_history = qcnn_model.fit(train_images, train_labels,\n",
    "                        validation_split = 0.3, \n",
    "                        epochs=5, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(img_size = 28,\n",
    "              channels = 1):\n",
    "    cnn_model = models.Sequential([\n",
    "        layers.Conv2D(filters=32, kernel_size=2, activation='relu', strides = (1,1), input_shape=(img_size, img_size, channels), data_format='channels_last'),\n",
    "        #layers.MaxPooling2D((2, 2), strides = (2,2)),\n",
    "        layers.Conv2D(filters=64, kernel_size=2, activation='relu', strides = (2,2)),\n",
    "        layers.Conv2D(filters=64, kernel_size=2, activation='relu', strides = (2,2)),\n",
    "        #layers.MaxPooling2D((2, 2), strides = (2,2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')]\n",
    "    )\n",
    "\n",
    "    cnn_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = cnn_model(\n",
    "    img_size = 10\n",
    ")\n",
    "n_epochs = 10\n",
    "#steps_per_epoch = 40\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_history = cnn.fit(train_images, train_labels,\n",
    "                            #steps_per_epoch = steps_per_epoch,\n",
    "                            #validation_data=(valid_images, valid_labels),\n",
    "                            validation_split = 0.3,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=n_epochs, \n",
    "                            verbose=2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
