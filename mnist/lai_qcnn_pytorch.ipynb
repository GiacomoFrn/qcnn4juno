{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QCNN tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "colors = [\n",
    "    \"#7eb0d5\",\n",
    "    \"#fd7f6f\",\n",
    "    \"#b2e061\",\n",
    "    \"#bd7ebe\",\n",
    "    \"#ffb55a\",\n",
    "    \"#8bd3c7\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import distributions\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class QuantumConv2d(nn.Conv2d):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 eps,\n",
    "                 cap,\n",
    "                 ratio,\n",
    "                 delta,\n",
    "                 stride=1,\n",
    "                 padding=0,\n",
    "                 dilation=1):\n",
    "        \"\"\"Base class for quantum convolutional layer.\n",
    "        \n",
    "        Args:\n",
    "            in_channels (int): number of input channels.\n",
    "            out_channels (int): number of output channels.\n",
    "            kernel_size (int): size of the convolution kernel.\n",
    "            eps (float): precision of quantum multiplication.\n",
    "            cap (float): value for cap 'relu' activation function.\n",
    "            ratio (float): precision of quantum tomography.\n",
    "            delta (float): precision of quantum gradient estimation.\n",
    "            stride (int, optional): convolution stride. Defaults to 1.\n",
    "            padding (int, optional): convolution padding. Defaults to 0.\n",
    "            dilation (int, optional): convolution dilation. Defaults to 1.\n",
    "        \"\"\"\n",
    "        # convolution layer\n",
    "        super(QuantumConv2d, self).__init__(in_channels,\n",
    "                                            out_channels,\n",
    "                                            kernel_size,\n",
    "                                            stride=stride,\n",
    "                                            padding=padding,\n",
    "                                            dilation=dilation,\n",
    "                                            groups=1,\n",
    "                                            bias=False,\n",
    "                                            padding_mode='zeros')\n",
    "\n",
    "        # set/check quantum parameters\n",
    "        self.set_quantum_params(eps, cap, ratio, delta)\n",
    "\n",
    "        # unfold operation\n",
    "        self.unfold = nn.Unfold(kernel_size, dilation, padding, stride)\n",
    "\n",
    "        # gradient operation\n",
    "        self.weight.register_hook(self.simulate_quantum_gradient)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        # get convolutional layer output\n",
    "        output = super(QuantumConv2d, self).forward(input)\n",
    "        output = torch.clamp(output, 0., self.cap)\n",
    "\n",
    "        # get quantum output\n",
    "        quantum_output = self.simulate_quantum_output(input, output)\n",
    "\n",
    "        # update convolutional layer output\n",
    "        output.data = quantum_output.data\n",
    "\n",
    "        return output\n",
    "\n",
    "    def simulate_quantum_output(self, input, output):\n",
    "        with torch.no_grad():\n",
    "            if self.eps > 0.:\n",
    "                # get kernel norm\n",
    "                kernel_matrix = self.weight.data.flatten(start_dim=1)\n",
    "                kernel_matrix = kernel_matrix.transpose(0, 1)\n",
    "                kernel_norm = torch.norm(kernel_matrix, dim=0)\n",
    "                kernel_norm = kernel_norm.repeat(input.size(0), 1, 1)\n",
    "\n",
    "                # get input norm\n",
    "                input_matrix = self.unfold(input)\n",
    "                input_matrix = input_matrix.transpose(1, 2)\n",
    "                input_norm = torch.norm(input_matrix, dim=2)\n",
    "                input_norm = input_norm.unsqueeze(2)\n",
    "\n",
    "                # add gaussian noise\n",
    "                product_norm = torch.bmm(input_norm, kernel_norm)\n",
    "                product_norm = product_norm.reshape(output.shape)\n",
    "                noise = torch.randn(output.shape, device=output.device)\n",
    "                output += 2 * self.eps * product_norm * noise\n",
    "                output = torch.clamp(output, 0., self.cap)\n",
    "\n",
    "            if self.ratio < 1.:\n",
    "                # quantum sampling\n",
    "                num_samples = int(self.ratio * output.shape[1:].numel())\n",
    "                probs = output.flatten(start_dim=1)\n",
    "                distribution = distributions.Categorical(probs=probs)\n",
    "                samples = distribution.sample((num_samples, )).flatten()\n",
    "                idxs = torch.arange(0, input.size(0))\n",
    "                idxs = idxs.repeat(1, num_samples).flatten()\n",
    "                mask = torch.zeros_like(probs)\n",
    "                mask[idxs, samples] = 1.\n",
    "                mask = mask.reshape(output.shape)\n",
    "                output = mask * output\n",
    "        return output\n",
    "\n",
    "    def simulate_quantum_gradient(self, grad):\n",
    "        if self.delta > 0.:\n",
    "            # add quantum gradient estimation error to the kernel\n",
    "            noise = torch.randn(grad.shape, device=grad.device)\n",
    "            grad_norm = torch.norm(grad)\n",
    "            error = self.delta * grad_norm * noise\n",
    "            grad += error\n",
    "        return grad\n",
    "\n",
    "    def set_quantum_params(self, eps=None, cap=None, ratio=None, delta=None):\n",
    "        if eps is not None:\n",
    "            assert 0. <= eps <= 1., 'epsilon should verify: 0.<=eps<=1.'\n",
    "            self.eps = eps\n",
    "        if cap is not None:\n",
    "            assert 0. < cap, 'cap should verify: 0.<cap'\n",
    "            self.cap = cap\n",
    "        if ratio is not None:\n",
    "            assert 0. < ratio <= 1., 'ratio should verify: 0.<ratio<=1.'\n",
    "            self.ratio = ratio\n",
    "        if delta is not None:\n",
    "            assert 0 <= delta <= 1., 'delta should verify: 0.<=delta<=1.'\n",
    "            self.delta = delta\n",
    "\n",
    "    def convert_to_classical(self):\n",
    "        layer = nn.Conv2d(self.in_channels,\n",
    "                          self.out_channels,\n",
    "                          self.kernel_size,\n",
    "                          self.stride,\n",
    "                          self.padding,\n",
    "                          self.dilation,\n",
    "                          groups=1,\n",
    "                          bias=False,\n",
    "                          padding_mode='zeros')\n",
    "        layer.weight.data = self.weight.data\n",
    "        return layer\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = '{in_channels}, {out_channels}, kernel_size={kernel_size}, '\n",
    "        s += 'quantum_eps={eps}, quantum_cap = {cap}, '\n",
    "        s += 'quantum_ratio={ratio}, quantum_delta = {delta}, '\n",
    "        s += 'stride={stride}'\n",
    "        if self.padding != (0, ) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1, ) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        return s.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QCNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            QuantumConv2d(\n",
    "                in_channels  = 1,              \n",
    "                out_channels = 32,            \n",
    "                kernel_size  = 3,              \n",
    "                stride       = 1,                   \n",
    "                padding      = 0,  \n",
    "                eps          = 0.01,\n",
    "                cap          = 10,\n",
    "                ratio        = 0.5,\n",
    "                delta        = 0.01                \n",
    "            ),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),    \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels  = 32,              \n",
    "                out_channels = 64,            \n",
    "                kernel_size  = 3,              \n",
    "                stride       = 1,                   \n",
    "                padding      = 0,                  \n",
    "            ),   \n",
    "            nn.ReLU(),                                      \n",
    "        )\n",
    "        self.conv3 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels  = 64,              \n",
    "                out_channels = 64,            \n",
    "                kernel_size  = 3,              \n",
    "                stride       = 1,                   \n",
    "                padding      = 0,                  \n",
    "            ),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),    \n",
    "        )\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.dense = nn.Linear(64 * 4 * 4, 100)\n",
    "        self.out = nn.Linear(100, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        # flatten the output of conv3 to (batch_size, n_neurons)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.dense(x)      \n",
    "        output = self.out(x)\n",
    "        return output, x    # return x for visualization\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,                         \n",
    "    transform = ToTensor(),  \n",
    "    download = True           \n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data', \n",
    "    train = False, \n",
    "    transform = ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "\n",
      "\n",
      "training data shape:\t torch.Size([60000, 28, 28])\n",
      "test data shape:\t torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print()\n",
    "print(test_data)\n",
    "print()\n",
    "print()\n",
    "print(\"training data shape:\\t\", train_data.data.shape)\n",
    "print(\"test data shape:\\t\", test_data.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(9):\n",
    "    ax = fig.add_subplot(330 + 1 + i)\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(train_data.data[i], cmap=\"gray\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x29219d6d0>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x29219d850>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 30\n",
    "loaders = {\n",
    "    'train' : torch.utils.data.DataLoader(\n",
    "        train_data, \n",
    "        batch_size  = BATCH_SIZE, \n",
    "        shuffle     = True, \n",
    "        num_workers = 1\n",
    "    ),\n",
    "    'test'  : torch.utils.data.DataLoader(\n",
    "        test_data, \n",
    "        batch_size  = BATCH_SIZE, \n",
    "        shuffle     = True, \n",
    "        num_workers = 1\n",
    "    ),\n",
    "}\n",
    "loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([30, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([30])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 4\n"
     ]
    }
   ],
   "source": [
    "# use DataLoader to randomly pick batches of data\n",
    "train_features, train_labels = next(iter(loaders[\"train\"]))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that computes the output shape of a convolutional or pooling layer\n",
    "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    \"\"\"\n",
    "    Utility function for computing output of convolutions\n",
    "    takes a tuple of (h,w) and returns a tuple of (h,w)\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(h_w) is not tuple:\n",
    "        h_w = (h_w, h_w)\n",
    "    \n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    \n",
    "    if type(stride) is not tuple:\n",
    "        stride = (stride, stride)\n",
    "    \n",
    "    if type(pad) is not tuple:\n",
    "        pad = (pad, pad)\n",
    "    \n",
    "    h = (h_w[0] + (2 * pad[0]) - (dilation * (kernel_size[0] - 1)) - 1)// stride[0] + 1\n",
    "    w = (h_w[1] + (2 * pad[1]) - (dilation * (kernel_size[1] - 1)) - 1)// stride[1] + 1\n",
    "    \n",
    "    return h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QCNN()\n",
    "\n",
    "loss_fn   = nn.CrossEntropyLoss()   \n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)   \n",
    "\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer, n_batches, verbose=False, save=False):\n",
    "    running_loss  = 0.\n",
    "    last_loss     = 0.\n",
    "    total_correct = 0.\n",
    "\n",
    "    log_batches = int(n_batches / 10)\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(loaders[\"train\"]):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs, _ = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_correct += get_num_correct(outputs, labels)\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % log_batches == log_batches-1:\n",
    "            last_loss = running_loss / log_batches # average loss per batch (averaged over log_batches batches)\n",
    "            avg_acc   = total_correct / (len(inputs) * i)\n",
    "            if verbose: print(f'  batch {i + 1} loss: {last_loss}')\n",
    "            if save:\n",
    "                tb_x = epoch_index * len(loaders[\"train\"]) + i + 1\n",
    "                tb_writer.add_scalar('batch_loss', last_loss, tb_x)\n",
    "                tb_writer.add_scalar('batch_accuracy', avg_acc, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss, total_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer    = SummaryWriter(log_dir=f'torch_logs/{timestamp}')\n",
    "train_writer    = SummaryWriter(log_dir=f'torch_logs/{timestamp}/train')\n",
    "valid_writer    = SummaryWriter(log_dir=f'torch_logs/{timestamp}/validation')\n",
    "\n",
    "EPOCHS    = 10\n",
    "n_batches = len(loaders[\"train\"])\n",
    "\n",
    "images, labels = next(iter(loaders[\"train\"]))\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "writer.add_image(\"images\", grid)\n",
    "writer.add_graph(model, images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting EPOCH 1\n",
      "starting EPOCH 2\n",
      "starting EPOCH 3\n",
      "starting EPOCH 4\n",
      "starting EPOCH 5\n",
      "starting EPOCH 6\n",
      "starting EPOCH 7\n",
      "starting EPOCH 8\n",
      "starting EPOCH 9\n",
      "starting EPOCH 10\n",
      "LOSS train 0.1689699151692912 valid 0.11693553626537323\n",
      "ACCURACY train 0.95465 valid 0.9652\n"
     ]
    }
   ],
   "source": [
    "best_vloss = 1_000_000.\n",
    "save_model = False\n",
    "save_loss  = True\n",
    "verbose    = False\n",
    "\n",
    "for epoch_number, _ in enumerate(range(EPOCHS)):\n",
    "    print(f'starting EPOCH {epoch_number + 1}')\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss, total_correct = train_one_epoch(epoch_number, train_writer, n_batches, verbose=verbose, save=save_loss)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    total_vcorrect = 0.\n",
    "    for i, vdata in enumerate(loaders[\"test\"]):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs, _      = model(vinputs)\n",
    "        vloss            = loss_fn(voutputs, vlabels)\n",
    "        running_vloss   += vloss\n",
    "        total_vcorrect += get_num_correct(voutputs, vlabels)\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    if verbose: print(f'LOSS train {avg_loss} valid {avg_vloss}')\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    \n",
    "    train_writer.add_scalar(\"epoch_loss\",     avg_loss, epoch_number)\n",
    "    train_writer.add_scalar(\"epoch_accuracy\", total_correct  / len(train_data), epoch_number)\n",
    "    valid_writer.add_scalar(\"epoch_loss\",     avg_vloss, epoch_number)\n",
    "    valid_writer.add_scalar(\"epoch_accuracy\", total_vcorrect / len(test_data), epoch_number)\n",
    "\n",
    "    writer.flush()\n",
    "    \n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "\n",
    "print(f'LOSS train {avg_loss} valid {avg_vloss}')\n",
    "print(f'ACCURACY train {total_correct / len(train_data)} valid {total_vcorrect / len(test_data)}')\n",
    "\n",
    "if save_model:\n",
    "    model_path = f'models/lai_torch_model_{timestamp}' \n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "91e94dacc13bcc9ab92871d8572d4c8a018895bdfb2528d081f4892614b17d07"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
